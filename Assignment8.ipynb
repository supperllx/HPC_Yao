{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can import *ANYTHING* you want here.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # this is just a tool to show a progress bar as your simulations are running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Nested Spheres\n",
    "\n",
    "Simulation is an incredibly useful tool in data science.  We can use simulation to evaluate how algorithms perform against ground truth, and how algorithms compare to one another.\n",
    "\n",
    "In this assignment, you will be implementing and extending the nested spheres simulation study found in *Elements of Statistical Learning* page 339. https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "# Nested Spheres\n",
    "\n",
    "Consider a dataset which contains 10 features $X_1 \\,, X_2 \\,, \\cdots \\,, X_{10}$.  The features are standard independent Gaussian random variables.  That is to say\n",
    "\n",
    "$$ X_j \\sim \\operatorname{Normal}(0,1) \\quad \\forall j = 1 \\dots 10$$\n",
    "\n",
    "We are going to use these features to study a classification problem.  You will have to create the target variable, $Y$ by computing the following rule:\n",
    "\n",
    "$$ Y = \\begin{cases}  1 \\quad \\mbox{ if } \\sum_{j=1}^{10} X^2_j>9.34 \\\\ 0 \\quad  \\mbox{else} \\end{cases}$$\n",
    "\n",
    "# The Simulation Study\n",
    "\n",
    "Follow these steps to complete the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 ( X / 25 pts )\n",
    "Write a function, `generate_data`, that takes a dataset size N and creates a dataset according to the rule above, returning the feature matrix `X` and the labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    # Create feature matrix X and labels y\n",
    "    X = np.random.normal(0, 1, size = (N, 10))\n",
    "    y = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        y[i] = 1 if np.sum(X[i, :]*X[i, :])>9.34 else 0\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 ( X / 25 pts )\n",
    "\n",
    "Write a function `run_simulation` that accepts two numbers, Ntrain and Ntest. It should generate a training set and testing set using your `generate_data` function and then train **four classifiers**. The first should be a bagged decision tree, the second should be a random forest with `max_features=1`, the third should be a random forest with `max_features=3`, and the fourth can be anything you like, for example a random forest with more features or an XGboost model. Use 500 trees in your random forests and the bagged classifier. The function should return the accuracy for each of these models estimated using the training set you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(NtrainX, Ntrainy, NtestX, Ntesty):\n",
    "    \n",
    "    # Code to generate data and run one simulation\n",
    "    bag = BaggingClassifier(DecisionTreeClassifier(max_depth = 5), n_estimators=500, n_jobs = 28)\n",
    "    rf_mf1 = RandomForestClassifier(n_estimators=500, # Number of trees to train\n",
    "                       criterion='gini', # How to train the trees. Also supports entropy.\n",
    "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
    "                       min_samples_split=2, # Minimum samples to create a split.\n",
    "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
    "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
    "                       max_features=1, # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
    "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
    "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-3.\n",
    "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false.\n",
    "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
    "                       n_jobs=28, # Parallel processing. Set to -1 for all cores. Watch your RAM!!\n",
    "                       random_state=0, # Seed\n",
    "                       verbose=0, # If to give info during training. Set to 0 for silent training.\n",
    "                       warm_start=False, # If train over previously trained tree.\n",
    "                       class_weight='balanced')\n",
    "    rf_mf3 = RandomForestClassifier(n_estimators=500, # Number of trees to train\n",
    "                       criterion='gini', # How to train the trees. Also supports entropy.\n",
    "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
    "                       min_samples_split=2, # Minimum samples to create a split.\n",
    "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
    "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
    "                       max_features=3, # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
    "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
    "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-3.\n",
    "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false.\n",
    "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
    "                       n_jobs=28, # Parallel processing. Set to -1 for all cores. Watch your RAM!!\n",
    "                       random_state=0, # Seed\n",
    "                       verbose=0, # If to give info during training. Set to 0 for silent training.\n",
    "                       warm_start=False, # If train over previously trained tree.\n",
    "                       class_weight='balanced')\n",
    "    my_classifier = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
    "                            learning_rate=0.05,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=500,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=28,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=1,                  # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=0,        # Seed\n",
    "                            missing=None)\n",
    "   \n",
    "    models = [bag, rf_mf1, rf_mf3, my_classifier]\n",
    "    predictions = np.zeros((NtestX.shape[0], len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(NtrainX, Ntrainy)\n",
    "        predictions[:, i] = model.predict(NtestX)\n",
    "        \n",
    "    bag_accuracy = accuracy_score(Ntesty, predictions[:,0])\n",
    "    rf_mf1_accuracy = accuracy_score(Ntesty, predictions[:,1])\n",
    "    rf_mf3_accuracy = accuracy_score(Ntesty, predictions[:,2])\n",
    "    my_classifier_accuracy = accuracy_score(Ntesty, predictions[:,3])\n",
    "    \n",
    "    return bag_accuracy, rf_mf1_accuracy, rf_mf3_accuracy, my_classifier_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 ( X / 25 pts )\n",
    "\n",
    "Run 50 simulations using a training set of size 1000 and a test set of size 5000 and record all the results in four vectors, one for each type of model. *You should probably debug your code using smaller training and test set sized first because these will take a while. The full simulation takes 10 minutes on my old 2.8GHz core i5 laptop.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [01:54<00:44,  3.16s/it]"
     ]
    }
   ],
   "source": [
    "#Setup code to record results here:\n",
    "Nsim=50\n",
    "accuracy50 = np.zeros((Nsim, 4))\n",
    "\n",
    "#Loop to run simulations:\n",
    "for sim in tqdm(range(Nsim)):\n",
    "    # Run simulations, collect data\n",
    "    trainX, trainy = generate_data(1000)\n",
    "    testX, testy = generate_data(5000)\n",
    "    accuracy50[sim, :] = run_simulation(trainX, trainy, testX, testy)\n",
    "    \n",
    "print(accuracy50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (X / 25 pts) \n",
    "Plot the error rates for each model using a boxplot for each one. The four models should be across the x-axis, and the y-axis should be accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the error rates as a box plot by model to complete the assignment.\n",
    "plt.boxplot(accuracy50, labels=['bagged_decision_tree', 'rf_mf1', 'rf_mf3', 'xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
